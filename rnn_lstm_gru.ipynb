{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = 'hello'\n",
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.551338195800781\n",
      "Clip gradient :  2.6076437372619043\n",
      "2.360447406768799\n",
      "Clip gradient :  0.5936792793439414\n",
      "1.9433908462524414\n",
      "Clip gradient :  0.40816742802189604\n",
      "1.8922014236450195\n",
      "Clip gradient :  0.09037988718327744\n",
      "1.8702802658081055\n",
      "Clip gradient :  0.2018156214094005\n",
      "1.828260898590088\n",
      "Clip gradient :  0.14315087040274327\n",
      "1.2632418870925903\n",
      "Clip gradient :  2.8536247345751056\n",
      "0.6201820373535156\n",
      "Clip gradient :  4.0141089325610775\n",
      "0.28866147994995117\n",
      "Clip gradient :  1.0504302915472385\n",
      "0.08379793167114258\n",
      "Clip gradient :  0.3003839774364803\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#    print(\"Params : \")\n",
    "#    num_params = 0\n",
    "#    for item in rnn.parameters():\n",
    "#        num_params += 1\n",
    "#        print(item.grad)\n",
    "#    print(\"NumParams :\", num_params)\n",
    "#    print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Написать реализацию LSTM и обучить предсказывать слово\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(LSTM, self).__init__() \n",
    "        \n",
    "        self.ix = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.ih = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.fx = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.fh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.gx = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.gh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.ox = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.oh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh  = nn.Tanh()\n",
    "        \n",
    "        self.outweight = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_h, prev_c):\n",
    "        \n",
    "        i = self.sigmoid(self.ix(x) + self.ih(prev_h))\n",
    "        f = self.sigmoid(self.fx(x) + self.fh(prev_h))\n",
    "        g = self.tanh(self.gx(x) + self.gh(prev_h))\n",
    "        o = self.sigmoid(self.ox(x) + self.oh(prev_h))\n",
    "        \n",
    "        c = f * prev_c + i * g\n",
    "        h = o * self.tanh(c)\n",
    "        output = self.outweight(h)\n",
    "        \n",
    "        return output, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn_lstm = LSTM(in_size=ds.vec_size, hidden_size=6, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn_lstm.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.47293853759766\n",
      "60.48529815673828\n",
      "41.61228942871094\n",
      "62.177001953125\n",
      "28.952451705932617\n",
      "16.255210876464844\n",
      "10.616647720336914\n",
      "6.8214850425720215\n",
      "4.902827739715576\n",
      "3.45855712890625\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = False\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn_lstm.ih.in_features)\n",
    "    cc = torch.zeros(rnn_lstm.ih.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh, cc = rnn_lstm(x, hh, cc)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn_lstm.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn_lstm.parameters(), max_norm=1)\n",
    "            \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn_lstm.eval()\n",
    "hh = torch.zeros(rnn_lstm.ih.in_features)\n",
    "cc = torch.zeros(rnn_lstm.ih.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh, cc= rnn_lstm(x, hh, cc)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(GRU, self).__init__() \n",
    "        self.rx = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.rh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.zx = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.zh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.nx = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.nh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh  = nn.Tanh()\n",
    "        \n",
    "        self.outweight = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_h):\n",
    "        \n",
    "        r = self.sigmoid(self.rx(x) + self.rh(prev_h))\n",
    "        z = self.sigmoid(self.zx(x) + self.zh(prev_h))\n",
    "        n = self.tanh(self.nx(x) + r * self.nh(prev_h))\n",
    "         \n",
    "        h = (1 - z) * n + z * prev_h\n",
    "        output = self.outweight(h)\n",
    "        \n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn_gru = GRU(in_size=ds.vec_size, hidden_size=6, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn_gru.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.94506072998047\n",
      "34.48104476928711\n",
      "8.561725616455078\n",
      "3.512789726257324\n",
      "1.137862205505371\n",
      "0.21988677978515625\n",
      "0.11696147918701172\n",
      "0.08054351806640625\n",
      "0.06529712677001953\n",
      "0.05706977844238281\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = False\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn_gru.rh.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn_gru(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn_gru.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn_gru.parameters(), max_norm=1)\n",
    "            \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn_gru.eval()\n",
    "hh = torch.zeros(rnn_gru.rh.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn_gru(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
